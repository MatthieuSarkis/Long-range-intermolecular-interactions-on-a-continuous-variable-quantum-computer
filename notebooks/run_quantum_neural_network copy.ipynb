{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n",
        "# It can be customized to whatever you like\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import strawberryfields as sf\n",
        "from strawberryfields import ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def interferometer(params, q):\n",
        "    \"\"\"Parameterised interferometer acting on ``N`` modes.\n",
        "\n",
        "    Args:\n",
        "        params (list[float]): list of length ``max(1, N-1) + (N-1)*N`` parameters.\n",
        "\n",
        "            * The first ``N(N-1)/2`` parameters correspond to the beamsplitter angles\n",
        "            * The second ``N(N-1)/2`` parameters correspond to the beamsplitter phases\n",
        "            * The final ``N-1`` parameters correspond to local rotation on the first N-1 modes\n",
        "\n",
        "        q (list[RegRef]): list of Strawberry Fields quantum registers the interferometer\n",
        "            is to be applied to\n",
        "    \"\"\"\n",
        "    N = len(q)\n",
        "    theta = params[:N*(N-1)//2]\n",
        "    phi = params[N*(N-1)//2:N*(N-1)]\n",
        "    rphi = params[-N+1:]\n",
        "\n",
        "    if N == 1:\n",
        "        # the interferometer is a single rotation\n",
        "        ops.Rgate(rphi[0]) | q[0]\n",
        "        return\n",
        "\n",
        "    n = 0  # keep track of free parameters\n",
        "\n",
        "    # Apply the rectangular beamsplitter array\n",
        "    # The array depth is N\n",
        "    for l in range(N):\n",
        "        for k, (q1, q2) in enumerate(zip(q[:-1], q[1:])):\n",
        "            # skip even or odd pairs depending on layer\n",
        "            if (l + k) % 2 != 1:\n",
        "                ops.BSgate(theta[n], phi[n]) | (q1, q2)\n",
        "                n += 1\n",
        "\n",
        "    # apply the final local phase shifts to all modes except the last one\n",
        "    for i in range(max(1, N - 1)):\n",
        "        ops.Rgate(rphi[i]) | q[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(params, q):\n",
        "    \"\"\"CV quantum neural network layer acting on ``N`` modes.\n",
        "\n",
        "    Args:\n",
        "        params (list[float]): list of length ``2*(max(1, N-1) + N**2 + n)`` containing\n",
        "            the number of parameters for the layer\n",
        "        q (list[RegRef]): list of Strawberry Fields quantum registers the layer\n",
        "            is to be applied to\n",
        "    \"\"\"\n",
        "    N = len(q)\n",
        "    M = int(N * (N - 1)) + max(1, N - 1)\n",
        "\n",
        "    int1 = params[:M]\n",
        "    s = params[M:M+N]\n",
        "    int2 = params[M+N:2*M+N]\n",
        "    dr = params[2*M+N:2*M+2*N]\n",
        "    dp = params[2*M+2*N:2*M+3*N]\n",
        "    k = params[2*M+3*N:2*M+4*N]\n",
        "\n",
        "    # begin layer\n",
        "    interferometer(int1, q)\n",
        "\n",
        "    for i in range(N):\n",
        "        ops.Sgate(s[i]) | q[i]\n",
        "\n",
        "    interferometer(int2, q)\n",
        "\n",
        "    for i in range(N):\n",
        "        ops.Dgate(dr[i], dp[i]) | q[i]\n",
        "        ops.Kgate(k[i]) | q[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def init_weights(modes, layers, active_sd=0.0001, passive_sd=0.1):\n",
        "    \"\"\"Initialize a 2D TensorFlow Variable containing normally-distributed\n",
        "    random weights for an ``N`` mode quantum neural network with ``L`` layers.\n",
        "\n",
        "    Args:\n",
        "        modes (int): the number of modes in the quantum neural network\n",
        "        layers (int): the number of layers in the quantum neural network\n",
        "        active_sd (float): the standard deviation used when initializing\n",
        "            the normally-distributed weights for the active parameters\n",
        "            (displacement, squeezing, and Kerr magnitude)\n",
        "        passive_sd (float): the standard deviation used when initializing\n",
        "            the normally-distributed weights for the passive parameters\n",
        "            (beamsplitter angles and all gate phases)\n",
        "\n",
        "    Returns:\n",
        "        tf.Variable[tf.float32]: A TensorFlow Variable of shape\n",
        "        ``[layers, 2*(max(1, modes-1) + modes**2 + modes)]``, where the Lth\n",
        "        row represents the layer parameters for the Lth layer.\n",
        "    \"\"\"\n",
        "    # Number of interferometer parameters:\n",
        "    M = int(modes * (modes - 1)) + max(1, modes - 1)\n",
        "\n",
        "    # Create the TensorFlow variables\n",
        "    int1_weights = tf.random.normal(shape=[layers, M], stddev=passive_sd)\n",
        "    s_weights = tf.random.normal(shape=[layers, modes], stddev=active_sd)\n",
        "    int2_weights = tf.random.normal(shape=[layers, M], stddev=passive_sd)\n",
        "    dr_weights = tf.random.normal(shape=[layers, modes], stddev=active_sd)\n",
        "    dp_weights = tf.random.normal(shape=[layers, modes], stddev=passive_sd)\n",
        "    k_weights = tf.random.normal(shape=[layers, modes], stddev=active_sd)\n",
        "\n",
        "    weights = tf.concat(\n",
        "        [int1_weights, s_weights, int2_weights, dr_weights, dp_weights, k_weights], axis=1\n",
        "    )\n",
        "\n",
        "    weights = tf.Variable(weights)\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(137)\n",
        "np.random.seed(137)\n",
        "\n",
        "\n",
        "# define width and depth of CV quantum neural network\n",
        "modes = 2\n",
        "layers = 8\n",
        "cutoff_dim = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# defining desired state (single photon state in the first mode, vacuum in the remaining modes)\n",
        "# remark: it seems that in the convention of strawberryfield the tensor product of \n",
        "# two vector of size (d,) gives a matrix of size (d, d) instead of a vector of size (d*d,).\n",
        "target_state = np.zeros(shape=[cutoff_dim for _ in range(modes)])\n",
        "target_state[1, tuple(0 for _ in range(modes))] = 1\n",
        "target_state = tf.constant(target_state, dtype=tf.complex64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initialize engine and program\n",
        "eng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": cutoff_dim})\n",
        "qnn = sf.Program(modes)\n",
        "\n",
        "# initialize QNN weights\n",
        "weights = init_weights(modes, layers) # our TensorFlow weights\n",
        "num_params = np.prod(weights.shape)   # total number of parameters in our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/7j/z471krp14yq8nlbjn0hnpmp4y7whw6/T/ipykernel_34829/1819069251.py:3: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "# Create array of Strawberry Fields symbolic gate arguments, matching\n",
        "# the size of the weights Variable.\n",
        "sf_params = np.arange(num_params).reshape(weights.shape).astype(np.str)\n",
        "sf_params = np.array([qnn.params(*i) for i in sf_params])\n",
        "\n",
        "\n",
        "# Construct the symbolic Strawberry Fields program by\n",
        "# looping and applying layers to the program.\n",
        "with qnn.context as q:\n",
        "    for k in range(layers):\n",
        "        layer(sf_params[k], q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(weights):\n",
        "    # Create a dictionary mapping from the names of the Strawberry Fields\n",
        "    # symbolic gate parameters to the TensorFlow weight values.\n",
        "    mapping = {p.name: w for p, w in zip(sf_params.flatten(), tf.reshape(weights, [-1]))}\n",
        "\n",
        "    # run the engine\n",
        "    state = eng.run(qnn, args=mapping).state\n",
        "    \n",
        "    #x = tf.Variable(np.zeros(shape=(modes,)))\n",
        "    #p = tf.Variable(np.zeros(shape=(modes,)))\n",
        "\n",
        "    #ket = state.ket()\n",
        "    x0 = state.quad_expectation(mode=0, phi=0.0)[0] # returns both the expectation value and variance of the position quadrature.\n",
        "    p0 = state.quad_expectation(mode=0, phi=0.5*np.pi)[0] # returns both the expectation value and variance of the momentum quadrature.\n",
        "    x1 = state.quad_expectation(mode=1, phi=0.0)[0] # returns both the expectation value and variance of the position quadrature.\n",
        "    p1 = state.quad_expectation(mode=1, phi=0.5*np.pi)[0] # returns both the expectation value and variance of the momentum quadrature.\n",
        "\n",
        "    gamma = 1.0\n",
        "    H = 0.5 * (x0**2 + p0**2 + x1**2 + p1**2) + 0.5 * gamma * x0 * x1\n",
        "\n",
        "    return H\n",
        "\n",
        "    #difference = tf.reduce_sum(tf.abs(ket - target_state))\n",
        "    #fidelity = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state)) ** 2\n",
        "    #return difference, fidelity, ket, tf.math.real(state.trace())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rep: 0 Cost: 0.00002247026532131713\n",
            "Rep: 1 Cost: 0.00018807721789926291\n",
            "Rep: 2 Cost: 0.00002286284870933741\n",
            "Rep: 3 Cost: 0.00002889412280637771\n",
            "Rep: 4 Cost: 0.00009358508395962417\n",
            "Rep: 5 Cost: 0.00005959050758974627\n",
            "Rep: 6 Cost: 0.00000686653947923332\n",
            "Rep: 7 Cost: 0.00000852741050039185\n",
            "Rep: 8 Cost: 0.00004202855052426457\n",
            "Rep: 9 Cost: 0.00004783375698025338\n",
            "Rep: 10 Cost: 0.00002212949038948864\n",
            "Rep: 11 Cost: 0.00000117988031433924\n",
            "Rep: 12 Cost: 0.00000655705116514582\n",
            "Rep: 13 Cost: 0.00002381162266829051\n",
            "Rep: 14 Cost: 0.00002733967266976833\n",
            "Rep: 15 Cost: 0.00001413814607076347\n",
            "Rep: 16 Cost: 0.00000142586054607818\n",
            "Rep: 17 Cost: 0.00000240992221733904\n",
            "Rep: 18 Cost: 0.00001210954360431060\n",
            "Rep: 19 Cost: 0.00001632345811231062\n",
            "Rep: 20 Cost: 0.00001016673468257068\n",
            "Rep: 21 Cost: 0.00000184149757842533\n",
            "Rep: 22 Cost: 0.00000060011183222741\n",
            "Rep: 23 Cost: 0.00000590247873333283\n",
            "Rep: 24 Cost: 0.00000962486501521198\n",
            "Rep: 25 Cost: 0.00000702008310327074\n",
            "Rep: 26 Cost: 0.00000177154970515403\n",
            "Rep: 27 Cost: 0.00000010884663481647\n",
            "Rep: 28 Cost: 0.00000295856170851039\n",
            "Rep: 29 Cost: 0.00000565102891414426\n",
            "Rep: 30 Cost: 0.00000453580651083030\n",
            "Rep: 31 Cost: 0.00000131214756038389\n",
            "Rep: 32 Cost: 0.00000002738151039239\n",
            "Rep: 33 Cost: 0.00000166809945767454\n",
            "Rep: 34 Cost: 0.00000339551434080931\n",
            "Rep: 35 Cost: 0.00000275698971563543\n",
            "Rep: 36 Cost: 0.00000076586042041527\n",
            "Rep: 37 Cost: 0.00000002944969779151\n",
            "Rep: 38 Cost: 0.00000111114036371873\n",
            "Rep: 39 Cost: 0.00000210943608180969\n",
            "Rep: 40 Cost: 0.00000156402256834554\n",
            "Rep: 41 Cost: 0.00000033394979936929\n",
            "Rep: 42 Cost: 0.00000006879208314103\n",
            "Rep: 43 Cost: 0.00000084791292920272\n",
            "Rep: 44 Cost: 0.00000131841522943432\n",
            "Rep: 45 Cost: 0.00000078874012388042\n",
            "Rep: 46 Cost: 0.00000008302206566668\n",
            "Rep: 47 Cost: 0.00000014258571923165\n",
            "Rep: 48 Cost: 0.00000067780769086312\n",
            "Rep: 49 Cost: 0.00000077626486927329\n",
            "Rep: 50 Cost: 0.00000031238147357726\n",
            "Rep: 51 Cost: 0.00000000116155796093\n",
            "Rep: 52 Cost: 0.00000022013173861524\n",
            "Rep: 53 Cost: 0.00000050887013003376\n",
            "Rep: 54 Cost: 0.00000038544328617718\n",
            "Rep: 55 Cost: 0.00000006819981024364\n",
            "Rep: 56 Cost: 0.00000003350079680331\n",
            "Rep: 57 Cost: 0.00000025291930683125\n",
            "Rep: 58 Cost: 0.00000031834048286328\n",
            "Rep: 59 Cost: 0.00000013008491350774\n",
            "Rep: 60 Cost: 0.00000000009561373915\n",
            "Rep: 61 Cost: 0.00000009883640927910\n",
            "Rep: 62 Cost: 0.00000021133496375114\n",
            "Rep: 63 Cost: 0.00000013953184918591\n",
            "Rep: 64 Cost: 0.00000001372246849485\n",
            "Rep: 65 Cost: 0.00000002976962321100\n",
            "Rep: 66 Cost: 0.00000012308700547692\n",
            "Rep: 67 Cost: 0.00000011633666474609\n",
            "Rep: 68 Cost: 0.00000002700411627643\n",
            "Rep: 69 Cost: 0.00000000609490946601\n",
            "Rep: 70 Cost: 0.00000006665191421007\n",
            "Rep: 71 Cost: 0.00000008509857707395\n",
            "Rep: 72 Cost: 0.00000003036709017579\n",
            "Rep: 73 Cost: 0.00000000045315179387\n",
            "Rep: 74 Cost: 0.00000003512083424084\n",
            "Rep: 75 Cost: 0.00000005810050751620\n",
            "Rep: 76 Cost: 0.00000002691646017183\n",
            "Rep: 77 Cost: 0.00000000008161202270\n",
            "Rep: 78 Cost: 0.00000001885087286269\n",
            "Rep: 79 Cost: 0.00000003845532958735\n",
            "Rep: 80 Cost: 0.00000002087969264153\n",
            "Rep: 81 Cost: 0.00000000041887693314\n",
            "Rep: 82 Cost: 0.00000001077075673095\n",
            "Rep: 83 Cost: 0.00000002531259823968\n",
            "Rep: 84 Cost: 0.00000001487479828199\n",
            "Rep: 85 Cost: 0.00000000044816575451\n",
            "Rep: 86 Cost: 0.00000000674334632578\n",
            "Rep: 87 Cost: 0.00000001678697536533\n",
            "Rep: 88 Cost: 0.00000000994770843477\n",
            "Rep: 89 Cost: 0.00000000026883587023\n",
            "Rep: 90 Cost: 0.00000000468051331026\n",
            "Rep: 91 Cost: 0.00000001124097614991\n",
            "Rep: 92 Cost: 0.00000000625770280038\n",
            "Rep: 93 Cost: 0.00000000008540820134\n",
            "Rep: 94 Cost: 0.00000000356796547862\n",
            "Rep: 95 Cost: 0.00000000755742490810\n",
            "Rep: 96 Cost: 0.00000000365462260454\n",
            "Rep: 97 Cost: 0.00000000000423889049\n",
            "Rep: 98 Cost: 0.00000000289061774517\n",
            "Rep: 99 Cost: 0.00000000502838393146\n",
            "Rep: 100 Cost: 0.00000000192224036688\n",
            "Rep: 101 Cost: 0.00000000005128162825\n",
            "Rep: 102 Cost: 0.00000000239196662477\n",
            "Rep: 103 Cost: 0.00000000323382431944\n",
            "Rep: 104 Cost: 0.00000000085306017805\n",
            "Rep: 105 Cost: 0.00000000019169424381\n",
            "Rep: 106 Cost: 0.00000000195085769761\n",
            "Rep: 107 Cost: 0.00000000194641103235\n",
            "Rep: 108 Cost: 0.00000000027220631305\n",
            "Rep: 109 Cost: 0.00000000035777686369\n",
            "Rep: 110 Cost: 0.00000000151600043630\n",
            "Rep: 111 Cost: 0.00000000104644659604\n",
            "Rep: 112 Cost: 0.00000000003358592224\n",
            "Rep: 113 Cost: 0.00000000048301973532\n",
            "Rep: 114 Cost: 0.00000000108300624024\n",
            "Rep: 115 Cost: 0.00000000046318310298\n",
            "Rep: 116 Cost: 0.00000000000771809439\n",
            "Rep: 117 Cost: 0.00000000052448329013\n",
            "Rep: 118 Cost: 0.00000000068139999199\n",
            "Rep: 119 Cost: 0.00000000013972692448\n",
            "Rep: 120 Cost: 0.00000000007986364348\n",
            "Rep: 121 Cost: 0.00000000047239978596\n",
            "Rep: 122 Cost: 0.00000000035391853337\n",
            "Rep: 123 Cost: 0.00000000001287191187\n",
            "Rep: 124 Cost: 0.00000000015942169806\n",
            "Rep: 125 Cost: 0.00000000035041036739\n",
            "Rep: 126 Cost: 0.00000000013285818279\n",
            "Rep: 127 Cost: 0.00000000000806920676\n",
            "Rep: 128 Cost: 0.00000000019237700322\n",
            "Rep: 129 Cost: 0.00000000020487780916\n",
            "Rep: 130 Cost: 0.00000000002347809447\n",
            "Rep: 131 Cost: 0.00000000005015037691\n",
            "Rep: 132 Cost: 0.00000000016687402848\n",
            "Rep: 133 Cost: 0.00000000008412742725\n",
            "Rep: 134 Cost: 0.00000000000050198181\n",
            "Rep: 135 Cost: 0.00000000008220889247\n",
            "Rep: 136 Cost: 0.00000000010526318706\n",
            "Rep: 137 Cost: 0.00000000001647326546\n",
            "Rep: 138 Cost: 0.00000000001972042321\n",
            "Rep: 139 Cost: 0.00000000007993571777\n",
            "Rep: 140 Cost: 0.00000000004342286947\n",
            "Rep: 141 Cost: 0.00000000000009920404\n",
            "Rep: 142 Cost: 0.00000000003953478517\n",
            "Rep: 143 Cost: 0.00000000005104591055\n",
            "Rep: 144 Cost: 0.00000000000737227467\n",
            "Rep: 145 Cost: 0.00000000001076729157\n",
            "Rep: 146 Cost: 0.00000000003943489979\n",
            "Rep: 147 Cost: 0.00000000001902605330\n",
            "Rep: 148 Cost: 0.00000000000036831654\n",
            "Rep: 149 Cost: 0.00000000002147897311\n",
            "Rep: 150 Cost: 0.00000000002330140594\n",
            "Rep: 151 Cost: 0.00000000000193773903\n",
            "Rep: 152 Cost: 0.00000000000759293670\n",
            "Rep: 153 Cost: 0.00000000001958759890\n",
            "Rep: 154 Cost: 0.00000000000671367129\n",
            "Rep: 155 Cost: 0.00000000000107300692\n",
            "Rep: 156 Cost: 0.00000000001237524666\n",
            "Rep: 157 Cost: 0.00000000000946005774\n",
            "Rep: 158 Cost: 0.00000000000010820926\n",
            "Rep: 159 Cost: 0.00000000000583958359\n",
            "Rep: 160 Cost: 0.00000000000912865009\n",
            "Rep: 161 Cost: 0.00000000000151051394\n",
            "Rep: 162 Cost: 0.00000000000180721139\n",
            "Rep: 163 Cost: 0.00000000000686584426\n",
            "Rep: 164 Cost: 0.00000000000297354307\n",
            "Rep: 165 Cost: 0.00000000000019277841\n",
            "Rep: 166 Cost: 0.00000000000418234414\n",
            "Rep: 167 Cost: 0.00000000000353967940\n",
            "Rep: 168 Cost: 0.00000000000006195416\n",
            "Rep: 169 Cost: 0.00000000000204433519\n",
            "Rep: 170 Cost: 0.00000000000323870770\n",
            "Rep: 171 Cost: 0.00000000000048266155\n",
            "Rep: 172 Cost: 0.00000000000074232439\n",
            "Rep: 173 Cost: 0.00000000000247094012\n",
            "Rep: 174 Cost: 0.00000000000089285719\n",
            "Rep: 175 Cost: 0.00000000000014968274\n",
            "Rep: 176 Cost: 0.00000000000162473258\n",
            "Rep: 177 Cost: 0.00000000000107615620\n",
            "Rep: 178 Cost: 0.00000000000000142257\n",
            "Rep: 179 Cost: 0.00000000000092871652\n",
            "Rep: 180 Cost: 0.00000000000103666988\n",
            "Rep: 181 Cost: 0.00000000000005208490\n",
            "Rep: 182 Cost: 0.00000000000045602140\n",
            "Rep: 183 Cost: 0.00000000000086290090\n",
            "Rep: 184 Cost: 0.00000000000014813480\n",
            "Rep: 185 Cost: 0.00000000000018266557\n",
            "Rep: 186 Cost: 0.00000000000064468147\n",
            "Rep: 187 Cost: 0.00000000000021870488\n",
            "Rep: 188 Cost: 0.00000000000005244864\n",
            "Rep: 189 Cost: 0.00000000000044113449\n",
            "Rep: 190 Cost: 0.00000000000024443606\n",
            "Rep: 191 Cost: 0.00000000000000646186\n",
            "Rep: 192 Cost: 0.00000000000028021482\n",
            "Rep: 193 Cost: 0.00000000000023348142\n",
            "Rep: 194 Cost: 0.00000000000000083926\n",
            "Rep: 195 Cost: 0.00000000000016582064\n",
            "Rep: 196 Cost: 0.00000000000020159818\n",
            "Rep: 197 Cost: 0.00000000000000976547\n",
            "Rep: 198 Cost: 0.00000000000009169156\n",
            "Rep: 199 Cost: 0.00000000000016168244\n",
            "Rep: 200 Cost: 0.00000000000002002322\n",
            "Rep: 201 Cost: 0.00000000000004740370\n",
            "Rep: 202 Cost: 0.00000000000012301737\n",
            "Rep: 203 Cost: 0.00000000000002648343\n",
            "Rep: 204 Cost: 0.00000000000002266940\n",
            "Rep: 205 Cost: 0.00000000000009015021\n",
            "Rep: 206 Cost: 0.00000000000002870200\n",
            "Rep: 207 Cost: 0.00000000000000985581\n",
            "Rep: 208 Cost: 0.00000000000006404882\n",
            "Rep: 209 Cost: 0.00000000000002762180\n",
            "Rep: 210 Cost: 0.00000000000000373900\n",
            "Rep: 211 Cost: 0.00000000000004473155\n",
            "Rep: 212 Cost: 0.00000000000002457914\n",
            "Rep: 213 Cost: 0.00000000000000118000\n",
            "Rep: 214 Cost: 0.00000000000003083629\n",
            "Rep: 215 Cost: 0.00000000000002070232\n",
            "Rep: 216 Cost: 0.00000000000000024026\n",
            "Rep: 217 Cost: 0.00000000000002113390\n",
            "Rep: 218 Cost: 0.00000000000001681341\n",
            "Rep: 219 Cost: 0.00000000000000001839\n",
            "Rep: 220 Cost: 0.00000000000001445171\n",
            "Rep: 221 Cost: 0.00000000000001322238\n",
            "Rep: 222 Cost: 0.00000000000000002746\n",
            "Rep: 223 Cost: 0.00000000000000994687\n",
            "Rep: 224 Cost: 0.00000000000001018594\n",
            "Rep: 225 Cost: 0.00000000000000006773\n",
            "Rep: 226 Cost: 0.00000000000000692885\n",
            "Rep: 227 Cost: 0.00000000000000767820\n",
            "Rep: 228 Cost: 0.00000000000000009499\n",
            "Rep: 229 Cost: 0.00000000000000483877\n",
            "Rep: 230 Cost: 0.00000000000000575015\n",
            "Rep: 231 Cost: 0.00000000000000009885\n",
            "Rep: 232 Cost: 0.00000000000000347705\n",
            "Rep: 233 Cost: 0.00000000000000423080\n",
            "Rep: 234 Cost: 0.00000000000000007728\n",
            "Rep: 235 Cost: 0.00000000000000253312\n",
            "Rep: 236 Cost: 0.00000000000000309230\n",
            "Rep: 237 Cost: 0.00000000000000004942\n",
            "Rep: 238 Cost: 0.00000000000000188021\n",
            "Rep: 239 Cost: 0.00000000000000222351\n",
            "Rep: 240 Cost: 0.00000000000000003013\n",
            "Rep: 241 Cost: 0.00000000000000142841\n",
            "Rep: 242 Cost: 0.00000000000000160813\n",
            "Rep: 243 Cost: 0.00000000000000001112\n",
            "Rep: 244 Cost: 0.00000000000000110243\n",
            "Rep: 245 Cost: 0.00000000000000112315\n",
            "Rep: 246 Cost: 0.00000000000000000166\n",
            "Rep: 247 Cost: 0.00000000000000085351\n",
            "Rep: 248 Cost: 0.00000000000000077322\n",
            "Rep: 249 Cost: 0.00000000000000000049\n",
            "Rep: 250 Cost: 0.00000000000000065936\n",
            "Rep: 251 Cost: 0.00000000000000052929\n",
            "Rep: 252 Cost: 0.00000000000000000471\n",
            "Rep: 253 Cost: 0.00000000000000051808\n",
            "Rep: 254 Cost: 0.00000000000000034861\n",
            "Rep: 255 Cost: 0.00000000000000001219\n",
            "Rep: 256 Cost: 0.00000000000000039548\n",
            "Rep: 257 Cost: 0.00000000000000022420\n",
            "Rep: 258 Cost: 0.00000000000000002034\n",
            "Rep: 259 Cost: 0.00000000000000030218\n",
            "Rep: 260 Cost: 0.00000000000000013622\n",
            "Rep: 261 Cost: 0.00000000000000002853\n",
            "Rep: 262 Cost: 0.00000000000000022570\n",
            "Rep: 263 Cost: 0.00000000000000007880\n",
            "Rep: 264 Cost: 0.00000000000000003473\n",
            "Rep: 265 Cost: 0.00000000000000017381\n",
            "Rep: 266 Cost: 0.00000000000000003926\n",
            "Rep: 267 Cost: 0.00000000000000004193\n",
            "Rep: 268 Cost: 0.00000000000000012457\n",
            "Rep: 269 Cost: 0.00000000000000001590\n",
            "Rep: 270 Cost: 0.00000000000000004383\n",
            "Rep: 271 Cost: 0.00000000000000008330\n",
            "Rep: 272 Cost: 0.00000000000000000497\n",
            "Rep: 273 Cost: 0.00000000000000004178\n",
            "Rep: 274 Cost: 0.00000000000000005629\n",
            "Rep: 275 Cost: 0.00000000000000000035\n",
            "Rep: 276 Cost: 0.00000000000000003825\n",
            "Rep: 277 Cost: 0.00000000000000003428\n",
            "Rep: 278 Cost: 0.00000000000000000038\n",
            "Rep: 279 Cost: 0.00000000000000003208\n",
            "Rep: 280 Cost: 0.00000000000000002006\n",
            "Rep: 281 Cost: 0.00000000000000000197\n",
            "Rep: 282 Cost: 0.00000000000000002802\n",
            "Rep: 283 Cost: 0.00000000000000000992\n",
            "Rep: 284 Cost: 0.00000000000000000438\n",
            "Rep: 285 Cost: 0.00000000000000002203\n",
            "Rep: 286 Cost: 0.00000000000000000375\n",
            "Rep: 287 Cost: 0.00000000000000000659\n",
            "Rep: 288 Cost: 0.00000000000000001575\n",
            "Rep: 289 Cost: 0.00000000000000000065\n",
            "Rep: 290 Cost: 0.00000000000000000773\n",
            "Rep: 291 Cost: 0.00000000000000000931\n",
            "Rep: 292 Cost: 0.00000000000000000001\n",
            "Rep: 293 Cost: 0.00000000000000000871\n",
            "Rep: 294 Cost: 0.00000000000000000492\n",
            "Rep: 295 Cost: 0.00000000000000000064\n",
            "Rep: 296 Cost: 0.00000000000000000695\n",
            "Rep: 297 Cost: 0.00000000000000000193\n",
            "Rep: 298 Cost: 0.00000000000000000163\n",
            "Rep: 299 Cost: 0.00000000000000000466\n",
            "Rep: 300 Cost: 0.00000000000000000047\n",
            "Rep: 301 Cost: 0.00000000000000000170\n",
            "Rep: 302 Cost: 0.00000000000000000311\n",
            "Rep: 303 Cost: 0.00000000000000000009\n",
            "Rep: 304 Cost: 0.00000000000000000200\n",
            "Rep: 305 Cost: 0.00000000000000000200\n",
            "Rep: 306 Cost: 0.00000000000000000004\n",
            "Rep: 307 Cost: 0.00000000000000000204\n",
            "Rep: 308 Cost: 0.00000000000000000099\n",
            "Rep: 309 Cost: 0.00000000000000000034\n",
            "Rep: 310 Cost: 0.00000000000000000192\n",
            "Rep: 311 Cost: 0.00000000000000000016\n"
          ]
        }
      ],
      "source": [
        "# set up the optimizer\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "cost_before = cost(weights)\n",
        "\n",
        "# Perform the optimization\n",
        "#for i in range(1000):\n",
        "for i in range(1000):\n",
        "    # reset the engine if it has already been executed\n",
        "    if eng.run_progs:\n",
        "        eng.reset()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = cost(weights)\n",
        "\n",
        "    # one repetition of the optimization\n",
        "    gradients = tape.gradient(loss, weights)\n",
        "    opt.apply_gradients(zip([gradients], [weights]))\n",
        "\n",
        "    # Prints progress at every rep\n",
        "    if i % 1 == 0:\n",
        "        print(\"Rep: {} Cost: {:.20f}\".format(i, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.15 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c6427a38b0cb3ed412f8db39e55d49c1a6e7316c2a3a4f60e70d9812fff22d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
